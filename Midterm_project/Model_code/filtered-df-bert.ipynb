{"cells":[{"cell_type":"code","execution_count":40,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-05T19:59:58.446557Z","iopub.status.busy":"2024-10-05T19:59:58.446125Z","iopub.status.idle":"2024-10-05T19:59:58.457020Z","shell.execute_reply":"2024-10-05T19:59:58.456137Z","shell.execute_reply.started":"2024-10-05T19:59:58.446517Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/filtered-llm/filtered_df_test.csv\n","/kaggle/input/filtered-llm/filtered_df_val.csv\n","/kaggle/input/filtered-llm/filtered_df_train.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:00.485358Z","iopub.status.busy":"2024-10-05T20:00:00.484447Z","iopub.status.idle":"2024-10-05T20:00:00.490466Z","shell.execute_reply":"2024-10-05T20:00:00.489574Z","shell.execute_reply.started":"2024-10-05T20:00:00.485307Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from transformers import AdamW, get_scheduler\n","import torch.nn.functional as F\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:02.581960Z","iopub.status.busy":"2024-10-05T20:00:02.581307Z","iopub.status.idle":"2024-10-05T20:00:02.587343Z","shell.execute_reply":"2024-10-05T20:00:02.586255Z","shell.execute_reply.started":"2024-10-05T20:00:02.581921Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:04.739116Z","iopub.status.busy":"2024-10-05T20:00:04.738435Z","iopub.status.idle":"2024-10-05T20:00:04.774130Z","shell.execute_reply":"2024-10-05T20:00:04.773199Z","shell.execute_reply.started":"2024-10-05T20:00:04.739074Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/input/filtered-llm/filtered_df_train.csv')\n","val_df = pd.read_csv('/kaggle/input/filtered-llm/filtered_df_val.csv')\n","test_df = pd.read_csv('/kaggle/input/filtered-llm/filtered_df_test.csv')\n"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:06.465566Z","iopub.status.busy":"2024-10-05T20:00:06.464735Z","iopub.status.idle":"2024-10-05T20:00:06.475079Z","shell.execute_reply":"2024-10-05T20:00:06.474006Z","shell.execute_reply.started":"2024-10-05T20:00:06.465512Z"},"trusted":true},"outputs":[],"source":["X_train = train_df['xi'] + \" \" + train_df['xj']\n","X_val = val_df['xi'] + \" \" + val_df['xj']\n","\n","y_train = train_df['LLM_model'].factorize()[0]\n","y_val = val_df['LLM_model'].factorize()[0]\n","\n"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:08.403676Z","iopub.status.busy":"2024-10-05T20:00:08.402800Z","iopub.status.idle":"2024-10-05T20:00:08.576918Z","shell.execute_reply":"2024-10-05T20:00:08.576098Z","shell.execute_reply.started":"2024-10-05T20:00:08.403627Z"},"trusted":true},"outputs":[],"source":["tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","#bert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(y_train))).to(device)\n"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:10.403177Z","iopub.status.busy":"2024-10-05T20:00:10.402793Z","iopub.status.idle":"2024-10-05T20:00:10.408456Z","shell.execute_reply":"2024-10-05T20:00:10.407310Z","shell.execute_reply.started":"2024-10-05T20:00:10.403141Z"},"trusted":true},"outputs":[],"source":["def encode_data(texts, tokenizer, max_len=100):\n","    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:12.077254Z","iopub.status.busy":"2024-10-05T20:00:12.076627Z","iopub.status.idle":"2024-10-05T20:00:18.973773Z","shell.execute_reply":"2024-10-05T20:00:18.972852Z","shell.execute_reply.started":"2024-10-05T20:00:12.077213Z"},"trusted":true},"outputs":[],"source":["encoded_train = encode_data(X_train, tokenizer, max_len=128)\n","encoded_val = encode_data(X_val, tokenizer, max_len=128)\n"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:21.007944Z","iopub.status.busy":"2024-10-05T20:00:21.007518Z","iopub.status.idle":"2024-10-05T20:00:21.013431Z","shell.execute_reply":"2024-10-05T20:00:21.012223Z","shell.execute_reply.started":"2024-10-05T20:00:21.007905Z"},"trusted":true},"outputs":[],"source":["X_train_bert = encoded_train['input_ids']\n","attention_mask_train = encoded_train['attention_mask']\n","X_val_bert = encoded_val['input_ids']\n","attention_mask_val = encoded_val['attention_mask']"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:21.763432Z","iopub.status.busy":"2024-10-05T20:00:21.763033Z","iopub.status.idle":"2024-10-05T20:00:21.769610Z","shell.execute_reply":"2024-10-05T20:00:21.768635Z","shell.execute_reply.started":"2024-10-05T20:00:21.763394Z"},"trusted":true},"outputs":[],"source":["class BertDataset(Dataset):\n","    def __init__(self, X, attention_mask, y):\n","        self.X = X\n","        self.attention_mask = attention_mask\n","        self.y = torch.tensor(y, dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.attention_mask[idx], self.y[idx]\n"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:22.541134Z","iopub.status.busy":"2024-10-05T20:00:22.540778Z","iopub.status.idle":"2024-10-05T20:00:22.547544Z","shell.execute_reply":"2024-10-05T20:00:22.546590Z","shell.execute_reply.started":"2024-10-05T20:00:22.541099Z"},"trusted":true},"outputs":[],"source":["train_dataset_bert = BertDataset(X_train_bert, attention_mask_train, y_train)\n","val_dataset_bert = BertDataset(X_val_bert, attention_mask_val, y_val)\n","train_loader_bert = DataLoader(train_dataset_bert, batch_size=64, shuffle=True)\n","val_loader_bert = DataLoader(val_dataset_bert, batch_size=64)\n"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:00:25.179307Z","iopub.status.busy":"2024-10-05T20:00:25.178912Z","iopub.status.idle":"2024-10-05T20:00:25.184042Z","shell.execute_reply":"2024-10-05T20:00:25.182974Z","shell.execute_reply.started":"2024-10-05T20:00:25.179258Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","from transformers import BertModel, get_scheduler\n"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:04:32.662997Z","iopub.status.busy":"2024-10-05T20:04:32.662582Z","iopub.status.idle":"2024-10-05T20:04:32.671420Z","shell.execute_reply":"2024-10-05T20:04:32.670514Z","shell.execute_reply.started":"2024-10-05T20:04:32.662958Z"},"trusted":true},"outputs":[],"source":["class EnhancedModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(EnhancedModel, self).__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')  \n","        self.dropout = nn.Dropout(p=0.2)  \n","        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)  \n","        self.fc2 = nn.Linear(512, 256)  \n","        self.classifier = nn.Linear(256, num_classes)  \n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]  \n","        x = self.dropout(pooled_output)\n","        x = F.relu(self.fc1(x))  \n","        x = F.relu(self.fc2(x))  \n","        x = self.dropout(x)\n","        return self.classifier(x)\n"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:04:35.304047Z","iopub.status.busy":"2024-10-05T20:04:35.303414Z","iopub.status.idle":"2024-10-05T20:04:35.688971Z","shell.execute_reply":"2024-10-05T20:04:35.688050Z","shell.execute_reply.started":"2024-10-05T20:04:35.304009Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["num_classes = len(set(y_train))\n","bert_model = EnhancedModel(num_classes).to(device)\n","\n","optimizer = AdamW(bert_model.parameters(), lr=3e-5, weight_decay=1e-4) \n","num_training_steps = len(train_loader_bert) * epochs \n","num_warmup_steps = int(0.1 * num_training_steps)  # 10% of total steps for warmup\n","lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T19:42:18.474251Z","iopub.status.busy":"2024-10-05T19:42:18.473256Z","iopub.status.idle":"2024-10-05T19:42:18.478574Z","shell.execute_reply":"2024-10-05T19:42:18.477582Z","shell.execute_reply.started":"2024-10-05T19:42:18.474205Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:04:42.601569Z","iopub.status.busy":"2024-10-05T20:04:42.600702Z","iopub.status.idle":"2024-10-05T20:04:42.615606Z","shell.execute_reply":"2024-10-05T20:04:42.614595Z","shell.execute_reply.started":"2024-10-05T20:04:42.601527Z"},"trusted":true},"outputs":[],"source":["def train_bert_model(model, train_loader, val_loader, optimizer, scheduler):\n","    patience = 3  \n","    epochs_no_improve = 0\n","    best_val_loss = float('inf')\n","    \n","    for param in model.bert.parameters():\n","        param.requires_grad = False\n","\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss, total_correct = 0, 0\n","\n","        for X_batch, attention_mask, y_batch in tqdm(train_loader):\n","            X_batch, attention_mask, y_batch = X_batch.to(device), attention_mask.to(device), y_batch.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(X_batch, attention_mask=attention_mask)  # Use your modified model\n","            loss = F.cross_entropy(outputs, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            total_loss += loss.item()\n","            total_correct += (outputs.argmax(1) == y_batch).sum().item()\n","\n","        accuracy = total_correct / len(train_loader.dataset)\n","        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}, Accuracy: {accuracy}\")\n","\n","        # Validate\n","        val_loss, val_accuracy, val_auc = validate_bert_model(model, val_loader)\n","        print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}, AUC-ROC: {val_auc}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve >= patience:\n","            print(\"Early stopping!\")\n","            break\n","\n","# Validation loop\n","def validate_bert_model(model, val_loader):\n","    model.eval()\n","    total_loss, total_correct = 0, 0\n","    all_targets = []\n","    all_probs = []\n","\n","    with torch.no_grad():\n","        for X_batch, attention_mask, y_batch in val_loader:\n","            X_batch, attention_mask, y_batch = X_batch.to(device), attention_mask.to(device), y_batch.to(device)\n","            outputs = model(X_batch, attention_mask=attention_mask)  \n","            loss = F.cross_entropy(outputs, y_batch)\n","\n","            total_loss += loss.item()\n","            total_correct += (outputs.argmax(1) == y_batch).sum().item()\n","\n","            all_targets.extend(y_batch.cpu().numpy())\n","            all_probs.extend(F.softmax(outputs, dim=1).cpu().numpy()) \n","\n","    val_accuracy = total_correct / len(val_loader.dataset)\n","    val_auc = roc_auc_score(all_targets, all_probs, multi_class='ovr')  \n","\n","    return total_loss / len(val_loader), val_accuracy, val_auc"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:04:44.810417Z","iopub.status.busy":"2024-10-05T20:04:44.809529Z","iopub.status.idle":"2024-10-05T20:05:57.615459Z","shell.execute_reply":"2024-10-05T20:05:57.614452Z","shell.execute_reply.started":"2024-10-05T20:04:44.810374Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:16<00:00,  3.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.6027566308066958, Accuracy: 0.20604395604395603\n","Validation Loss: 1.615304633975029, Validation Accuracy: 0.2, AUC-ROC: 0.50187\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:16<00:00,  3.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 1.5770942975604345, Accuracy: 0.27372627372627373\n","Validation Loss: 1.6329543441534042, Validation Accuracy: 0.186, AUC-ROC: 0.50071\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:15<00:00,  3.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 1.5494531922870212, Accuracy: 0.3101898101898102\n","Validation Loss: 1.650890588760376, Validation Accuracy: 0.2, AUC-ROC: 0.50065\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:15<00:00,  4.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 1.5265407070281014, Accuracy: 0.3194305694305694\n","Validation Loss: 1.6827496439218521, Validation Accuracy: 0.2, AUC-ROC: 0.502135\n","Early stopping!\n"]}],"source":["epochs = 10  \n","train_bert_model(bert_model, train_loader_bert, val_loader_bert, optimizer, lr_scheduler)\n"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:06:06.294681Z","iopub.status.busy":"2024-10-05T20:06:06.293820Z","iopub.status.idle":"2024-10-05T20:06:07.061257Z","shell.execute_reply":"2024-10-05T20:06:07.060305Z","shell.execute_reply.started":"2024-10-05T20:06:06.294636Z"},"trusted":true},"outputs":[],"source":["def prepare_test_data(test_df, tokenizer, max_len=100):\n","    X_test = test_df['xi'] + \" \" + test_df['xj']  \n","    y_test = test_df['LLM_model'].factorize()[0]  \n","\n","    encoded_test = encode_data(X_test, tokenizer, max_len)\n","    X_test_bert = encoded_test['input_ids']\n","    attention_mask_test = encoded_test['attention_mask']\n","\n","    test_dataset_bert = BertDataset(X_test_bert, attention_mask_test, y_test)\n","    return DataLoader(test_dataset_bert, batch_size=64), y_test  \n","\n","test_loader_bert, y_test = prepare_test_data(test_df, tokenizer, max_len=100)\n","\n","def evaluate_test_data(model, test_loader):\n","    model.eval()\n","    total_loss, total_correct = 0, 0\n","    all_targets = []\n","    all_probs = []\n","\n","    with torch.no_grad():\n","        for X_batch, attention_mask, y_batch in test_loader:\n","            X_batch, attention_mask, y_batch = X_batch.to(device), attention_mask.to(device), y_batch.to(device)\n","            outputs = model(X_batch, attention_mask=attention_mask)  # Use your modified model\n","            loss = F.cross_entropy(outputs, y_batch)\n","\n","            total_loss += loss.item()\n","            total_correct += (outputs.argmax(1) == y_batch).sum().item()\n","            all_targets.extend(y_batch.cpu().numpy())\n","            all_probs.extend(F.softmax(outputs, dim=1).cpu().numpy())\n","\n","    test_accuracy = total_correct / len(test_loader.dataset)\n","    test_loss = total_loss / len(test_loader)\n","    test_auc = roc_auc_score(all_targets, all_probs, multi_class='ovr')\n","\n","    return test_loss, test_accuracy, test_auc"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T20:06:09.150433Z","iopub.status.busy":"2024-10-05T20:06:09.149643Z","iopub.status.idle":"2024-10-05T20:06:10.786028Z","shell.execute_reply":"2024-10-05T20:06:10.785042Z","shell.execute_reply.started":"2024-10-05T20:06:09.150387Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 1.6798717826604843, Test Accuracy: 0.15568862275449102, Test AUC-ROC: 0.512026357373892\n"]}],"source":["test_loss, test_accuracy, test_auc = evaluate_test_data(bert_model, test_loader_bert)\n","print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Test AUC-ROC: {test_auc}\")"]},{"cell_type":"markdown","metadata":{},"source":["# OLD"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["weight_decay = 1e-5\n","patience = 3\n","#5e-5,1e-5(23), 2e-5(25), or 3e-5\n","optimizer = AdamW(bert_model.parameters(), lr=3e-5, weight_decay=weight_decay)\n","num_training_steps = len(train_loader_bert)  # Number of steps is based on the number of batches\n","lr_scheduler = get_scheduler(\n","    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_bert_model(model, train_loader, val_loader, optimizer, scheduler):\n","    global best_val_loss, epochs_no_improve  # Declare variables as global\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss, total_correct = 0, 0\n","        for X_batch, attention_mask, y_batch in tqdm(train_loader):\n","            X_batch, attention_mask, y_batch = X_batch.to(device), attention_mask.to(device), y_batch.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(X_batch, attention_mask=attention_mask).logits\n","            loss = F.cross_entropy(outputs, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            total_loss += loss.item()\n","            total_correct += (outputs.argmax(1) == y_batch).sum().item()\n","\n","        accuracy = total_correct / len(train_loader.dataset)\n","        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}, Accuracy: {accuracy}\")\n","\n","        # Validate\n","        val_loss, val_accuracy = validate_bert_model(model, val_loader)\n","        print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","            if epochs_no_improve >= patience:\n","                print(\"Early stopping!\")\n","                break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_val_loss = float('inf')\n","epochs_no_improve = 0\n","epochs = 10  # Define your number of epochs if needed\n","train_bert_model(bert_model, train_loader_bert, val_loader_bert, optimizer, lr_scheduler)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Hyperparameter tuning parameters\n","learning_rates = [1e-5, 2e-5, 3e-5, 5e-5]\n","batch_sizes = [16, 32, 64]\n","weight_decays = [0.0, 0.01, 0.1]  # Tuning weight decay\n","warmup_steps = [0, 100, 200]  # Tuning warm-up steps\n","num_epochs = 5 \n","\n","def hyperparameter_tuning(learning_rates, batch_sizes, weight_decays, warmup_steps):\n","    best_accuracy = 0\n","    best_params = {}\n","\n","    for lr, batch_size, weight_decay, warmup in itertools.product(learning_rates, batch_sizes, weight_decays, warmup_steps):\n","        print(f\"Training with lr={lr}, batch_size={batch_size}, weight_decay={weight_decay}, warmup_steps={warmup}\")\n","\n","        train_dataset_bert = BertDataset(X_train_bert, attention_mask_train, y_train)\n","        val_dataset_bert = BertDataset(X_val_bert, attention_mask_val, y_val)\n","        train_loader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n","        val_loader_bert = DataLoader(val_dataset_bert, batch_size=batch_size)\n","\n","        optimizer = AdamW(bert_model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","        num_training_steps = num_epochs * len(train_loader_bert)\n","        num_warmup_steps = warmup if warmup > 0 else int(0.1 * num_training_steps)  \n","\n","        lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n","\n","        def train_bert_model(model, train_loader, val_loader, optimizer, scheduler, epochs):\n","            for epoch in range(epochs):\n","                model.train()\n","                total_loss, total_correct = 0, 0\n","                for X_batch, attention_mask, y_batch in tqdm(train_loader):\n","                    X_batch, attention_mask, y_batch = X_batch.to(device), attention_mask.to(device), y_batch.to(device)\n","                    optimizer.zero_grad()\n","                    outputs = model(X_batch, attention_mask=attention_mask).logits\n","                    loss = F.cross_entropy(outputs, y_batch)\n","                    loss.backward()\n","                    optimizer.step()\n","                    scheduler.step()\n","\n","                    total_loss += loss.item()\n","                    total_correct += (outputs.argmax(1) == y_batch).sum().item()\n","\n","                accuracy = total_correct / len(train_loader.dataset)\n","                print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}, Accuracy: {accuracy}\")\n","\n","                # Validate\n","                val_loss, val_accuracy = validate_bert_model(model, val_loader)\n","                print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n","\n","        def validate_bert_model(model, val_loader):\n","            model.eval()\n","            total_loss, total_correct = 0, 0\n","            with torch.no_grad():\n","                for X_batch, attention_mask, y_batch in val_loader:\n","                    X_batch, attention_mask, y_batch = X_batch.to(device), attention_mask.to(device), y_batch.to(device)\n","                    outputs = model(X_batch, attention_mask=attention_mask).logits\n","                    loss = F.cross_entropy(outputs, y_batch)\n","                    total_loss += loss.item()\n","                    total_correct += (outputs.argmax(1) == y_batch).sum().item()\n","            \n","            val_accuracy = total_correct / len(val_loader.dataset)\n","            return total_loss / len(val_loader), val_accuracy\n","\n","        train_bert_model(bert_model, train_loader_bert, val_loader_bert, optimizer, lr_scheduler, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["hyperparameter_tuning(learning_rates, batch_sizes, weight_decays, warmup_steps)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5821812,"sourceId":9554487,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
